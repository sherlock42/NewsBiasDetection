{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import ast\n",
    "import operator\n",
    "from collections import Counter\n",
    "import statistics as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:\\\\Users\\\\Reen\\\\Desktop\\\\updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_entity(entity):\n",
    "    result = (entity.lower()).translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    word_list = nltk.word_tokenize(result)\n",
    "    filtered_output = [w for w in word_list if not w in stop_words]\n",
    "    lemmatized_key = [lemmatizer.lemmatize(w) for w in filtered_output]\n",
    "    return lemmatized_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_entity(match_dict,entity):\n",
    "    matched_key_dict = {}\n",
    "    entity_lemma = lemmatize_entity(str(entity))\n",
    "    for key in match_dict.keys():\n",
    "        key_lemma = lemmatize_entity(key)\n",
    "        if(len(list(set(key_lemma).intersection(set(entity_lemma))))>0):\n",
    "            matched_key_dict[key] = match_dict[key]\n",
    "            #return True\n",
    "    return matched_key_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_source_emotions(emotions_dict,tfidf_dict):\n",
    "    common_emotions_dict = {}\n",
    "    for key in tfidf_dict.keys():\n",
    "        lemma_tfidf_key = lemmatize_entity(key)\n",
    "        matched_key_dict = match_entity(emotions_dict,lemma_tfidf_key)\n",
    "        \n",
    "        emotion_avg = {}\n",
    "        #combine using avg\n",
    "        if(matched_key_dict):\n",
    "            emotion_values = (list(matched_key_dict.values()))\n",
    "            N = float(len(emotion_values))\n",
    "            emotion_avg = {emotion : sum(val[emotion] for val in emotion_values)/N for emotion in emotion_values[0]}\n",
    "            \n",
    "        #taking most representive mention with max words\n",
    "            key_lengths = {key : len(lemmatize_entity(key)) for key in matched_key_dict.keys()}\n",
    "            max_key = max(key_lengths.items(), key=operator.itemgetter(1))[0]\n",
    "            \n",
    "        #check\n",
    "            if(max_key not in common_emotions_dict):\n",
    "                common_emotions_dict[max_key] = emotion_avg\n",
    "\n",
    "    return common_emotions_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computed_sensational_score(src1_emotions, src2_emotions):\n",
    "   \n",
    "    common_emotions = []\n",
    "    allkeys_weighted_score = []\n",
    "    \n",
    "    for key in src1_emotions.keys():\n",
    "        weighted_score = {}\n",
    "        lemma_key_src1 = lemmatize_entity(key)\n",
    "        \n",
    "        #find matching keys in src2_emotions w.r.t. src1key\n",
    "        matched_key_dict = match_entity(src2_emotions,lemma_key_src1)\n",
    "        \n",
    "        #combine using avg\n",
    "        emotion_avg = {}\n",
    "        if(matched_key_dict):\n",
    "            emotion_values = (list(matched_key_dict.values()))\n",
    "            N = float(len(emotion_values))\n",
    "            emotion_avg = {emotion : sum(val[emotion] for val in emotion_values)/N for emotion in emotion_values[0]}\n",
    "        \n",
    "            key_score = src1_emotions[key]\n",
    "            matched_score = emotion_avg\n",
    "            \n",
    "            #calculate weighted score for \n",
    "            weighted_score = {key: abs(key_score[key] - matched_score.get(key, 0)) for key in key_score.keys()}\n",
    "            \n",
    "            #merge corresponding key and matched_key_dict to common_emotions -- [src1key,val,[matched_key_dict],emotion_avg]\n",
    "            common_emotions.append([key,key_score,matched_key_dict,matched_score])\n",
    "            allkeys_weighted_score.append(weighted_score)\n",
    "    \n",
    "    if(allkeys_weighted_score):\n",
    "        length = float(len(allkeys_weighted_score))\n",
    "        total_sensational_score = {emotion : sum(val[emotion] for val in allkeys_weighted_score)/length for emotion in allkeys_weighted_score[0]}\n",
    "        #print(total_sensational_score)\n",
    "        mean_score = stats.mean([v for k,v in total_sensational_score.items()])\n",
    "        #print(mean_score)\n",
    "        return [mean_score,total_sensational_score]\n",
    "    else:\n",
    "        return [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dd_emotions, ndtv_emotions, timesemotions, dd_tfidf, ndtv_tfidf, times_tfidf\n",
    "def process_rows(df):\n",
    "    \n",
    "    score_df = pd.DataFrame(columns=['ID','dd_ndtv_emotion_score','dd_times_emotion_score','dd_ndtv_emotion_mean_score','dd_times_emotion_mean_score'])\n",
    "    \n",
    "    for index,row in df.iterrows():\n",
    "        print(index)\n",
    "        dd_emotions = ast.literal_eval(row['dd_emotion'])\n",
    "        ndtv_emotions = ast.literal_eval(row['ndtv_emotion'])\n",
    "        times_emotions = ast.literal_eval(row['tn_emotion'])\n",
    "    \n",
    "        dd_tfidf = ast.literal_eval(row['dd_tfid'])\n",
    "        ndtv_tfidf = ast.literal_eval(row['ndtv_tfid'])\n",
    "        times_tfidf = ast.literal_eval(row['tn_tfid'])\n",
    "    \n",
    "    \n",
    "        dd_tfid_emotion_common = find_source_emotions(dd_emotions,dd_tfidf)\n",
    "        ndtv_tfid_emotion_common = find_source_emotions(ndtv_emotions,ndtv_tfidf)\n",
    "        times_tfid_emotion_common = find_source_emotions(times_emotions,times_tfidf)\n",
    "\n",
    "        score_dd_ndtv = computed_sensational_score(dd_tfid_emotion_common,ndtv_tfid_emotion_common)\n",
    "        sensational_score_dd_ndtv = score_dd_ndtv[1]\n",
    "        mean_score_dd_ndtv = score_dd_ndtv[0]\n",
    "        score_dd_times = computed_sensational_score(dd_tfid_emotion_common,times_tfid_emotion_common)\n",
    "        sensational_score_dd_times = score_dd_times[1]\n",
    "        mean_score_dd_times = score_dd_times[0]\n",
    "        score_df = score_df.append({'ID':row['ID'],'dd_ndtv_emotion_score':sensational_score_dd_ndtv,'dd_times_emotion_score':sensational_score_dd_times,'dd_ndtv_emotion_mean_score': mean_score_dd_ndtv,'dd_times_emotion_mean_score':mean_score_dd_times},ignore_index=True)\n",
    "    return score_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = process_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv('emotion_scores_1000_new.csv', encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

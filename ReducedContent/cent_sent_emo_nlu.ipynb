{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.helper_functions import does_contain,get_centralities,get_match,get_sent,sort_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = 'National'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.read_csv(f'../data/{cat}/factual.csv')\n",
    "ndtv = pd.read_csv(f'../data/{cat}/ndtv.csv')\n",
    "tn = pd.read_csv(f'../data/{cat}/timesNow.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GetDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Centrality Results/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start_count = 0\n",
    "if os.path.exists(file_name):\n",
    "    a = list(pd.read_csv(file_name)['ID'])\n",
    "    a.reverse()\n",
    "    start_count = a[0] + 1\n",
    "end_count = 251\n",
    "for id in tqdm_notebook(range(start_count,end_count)):\n",
    "    try:\n",
    "        ddtext = list(dd[dd['id'] == id].text.items())[0][1]\n",
    "        ndtvtext = list(ndtv[ndtv['id'] == id].text.items())[0][1]\n",
    "        tntext = list(tn[tn['id'] == id].text.items())[0][1]\n",
    "\n",
    "        ddtext = nlp(ddtext)._.coref_resolved\n",
    "        ndtvtext = nlp(ndtvtext)._.coref_resolved\n",
    "        tntext = nlp(tntext)._.coref_resolved\n",
    "\n",
    "        time.sleep(1)\n",
    "        dd_cent, dd_sent,dd_emotion = get_centralities(ddtext)\n",
    "        time.sleep(1)\n",
    "        ndtv_cent, ndtv_sent,ndtv_emotion = get_centralities(ndtvtext)\n",
    "        time.sleep(1)\n",
    "        tn_cent, tn_sent,tn_emotion = get_centralities(tntext)\n",
    "    except:\n",
    "        print(id)\n",
    "        continue\n",
    "    \n",
    "\n",
    "    dd_cent = sort_dict(dd_cent)\n",
    "    ndtv_cent = sort_dict(ndtv_cent)\n",
    "    tn_cent = sort_dict(tn_cent)\n",
    "\n",
    "    for i in range(len(dd_cent)):\n",
    "        dd_cent[i] = list(dd_cent[i])\n",
    "    for i in range(len(ndtv_cent)):\n",
    "        ndtv_cent[i] = list(ndtv_cent[i])\n",
    "    for i in range(len(tn_cent)):\n",
    "        tn_cent[i] = list(tn_cent[i])\n",
    "\n",
    "    shift_list = []\n",
    "    dd_tn = []\n",
    "    dd_ndtv = []\n",
    "    for dd_ent_rel in dd_cent:\n",
    "        dd_ent = dd_ent_rel[0].split(\" \")\n",
    "        tn_ent_rel = get_match(dd_ent,tn_cent)\n",
    "        ndtv_ent_rel = get_match(dd_ent,ndtv_cent)\n",
    "        if(tn_ent_rel[0] is None and ndtv_ent_rel[0] is None):\n",
    "            dd_ndtv.append(dd_ent_rel)\n",
    "            dd_tn.append(dd_ent_rel)\n",
    "        elif(tn_ent_rel[0] is None):\n",
    "            dd_tn.append(dd_ent_rel)\n",
    "        elif(ndtv_ent_rel[0] is None):\n",
    "            dd_ndtv.append(dd_ent_rel)\n",
    "        else:\n",
    "            shift_list.append([dd_ent_rel,ndtv_ent_rel,tn_ent_rel])\n",
    "\n",
    "    ndtv_dd = []\n",
    "    ndtv_tn = []\n",
    "    for ndtv_ent_rel in ndtv_cent:\n",
    "        ndtv_ent = ndtv_ent_rel[0].split(\" \")\n",
    "        tn_ent_rel = get_match(ndtv_ent,tn_cent)\n",
    "        dd_ent_rel = get_match(ndtv_ent,dd_cent)\n",
    "        if(tn_ent_rel[0] is None and dd_ent_rel[0] is None):\n",
    "            ndtv_dd.append(ndtv_ent_rel)\n",
    "            ndtv_tn.append(ndtv_ent_rel)\n",
    "        elif(tn_ent_rel[0] is None):\n",
    "            ndtv_tn.append(ndtv_ent_rel)\n",
    "        elif(dd_ent_rel[0] is None):\n",
    "            ndtv_dd.append(ndtv_ent_rel)   \n",
    "\n",
    "    tn_ndtv = []\n",
    "    tn_dd = []\n",
    "    for tn_ent_rel in tn_cent:\n",
    "        tn_ent = tn_ent_rel[0].split(\" \")\n",
    "        dd_ent_rel = get_match(tn_ent,dd_cent)\n",
    "        ndtv_ent_rel = get_match(tn_ent,ndtv_cent)\n",
    "        if(dd_ent_rel[0] is None and ndtv_ent_rel[0] is None):\n",
    "            tn_ndtv.append(tn_ent_rel)\n",
    "            tn_dd.append(tn_ent_rel)\n",
    "        elif(dd_ent_rel[0] is None):\n",
    "            tn_dd.append(tn_ent_rel)\n",
    "        elif(ndtv_ent_rel[0] is None):\n",
    "            tn_ndtv.append(tn_ent_rel)\n",
    "\n",
    "\n",
    "    for i in range(len(shift_list)):\n",
    "        for j in range(i + 1,len(shift_list)):\n",
    "            if(does_contain(shift_list[i][0][0],shift_list[j][0][0])):\n",
    "                shift_list[i][0][1] += shift_list[j][0][1]\n",
    "                shift_list[i][1][1] += shift_list[j][1][1]\n",
    "                shift_list[i][2][1] += shift_list[j][2][1]\n",
    "                shift_list[j][0][1] = 0\n",
    "                shift_list[j][1][1] = 0\n",
    "                shift_list[j][2][1] = 0\n",
    "    shift_list = [x for x in shift_list if x[0][1] != 0]\n",
    "\n",
    "    common_cent = {}\n",
    "    for tup in shift_list:\n",
    "        common_cent[tup[0][0]] = [tup[0][1],tup[1][1],tup[2][1]]\n",
    "\n",
    "    common_sent = {}\n",
    "    for key in common_cent:\n",
    "        common_sent[key] = [get_sent(key,dd_sent), get_sent(key,ndtv_sent), get_sent(key,tn_sent)]\n",
    "    \n",
    "    common_emo = {}\n",
    "    for key in common_cent:\n",
    "        common_emo[key] = [get_sent(key,dd_emotion),get_sent(key,ndtv_emotion),get_sent(key,tn_emotion)]\n",
    "\n",
    "    df = pd.DataFrame(columns=['ID','dd_text','ndtv_text','tn_text','dd_centralities','ndtv_centralities','tn_centralities','common_centralities','common_sentiment','common_emotion','dd_tn','dd_ndtv','tn_dd','tn_ndtv','ndtv_dd','ndtv_tn','dd_emotion','ndtv_emotion','tn_emotion'])\n",
    "    if os.path.exists(file_name):\n",
    "        df = pd.read_csv(file_name)\n",
    "    row =  {'ID': id,'dd_text': ddtext, 'ndtv_text':ndtvtext, 'tn_text' : tntext, 'common_centralities' : common_cent,'common_sentiment' : common_sent,\n",
    "            'dd_tn' : dd_tn, 'dd_ndtv' : dd_ndtv,'tn_dd' : tn_dd,'tn_ndtv' : tn_ndtv,\n",
    "            'ndtv_dd' : ndtv_dd,'ndtv_tn' : ndtv_tn,'dd_centralities' : dd_cent,'ndtv_centralities' : ndtv_cent,\n",
    "            'tn_centralities': tn_cent,'common_emotion' : common_emo,'dd_emotion' : dd_emotion,'ndtv_emotion': ndtv_emotion,'tn_emotion':tn_emotion}\n",
    "    df.loc[len(df)] = row\n",
    "    df.to_csv(file_name,index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
